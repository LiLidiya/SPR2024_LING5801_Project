{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f64f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165ab3fd",
   "metadata": {},
   "source": [
    "# Assignment 3: $N$-gram language models\n",
    "\n",
    "*Instructions*: Download this notebook and write code to answer the questions asso-\n",
    "ciated with the blank code blocks below. Upload your modified notebook using the\n",
    "Canvas link by **11:59pm on Tuesday, March 19**.\n",
    "\n",
    "Some questions ask you to provide written responses in markdown cells of the notebook.  Do not worry about formatting these cells neatly. Just provided the requested information in the indicated space.  If you want additional information on Jupyter markdown, you can refer to this __[cheat sheet](https://www.kaggle.com/code/cuecacuela/the-ultimate-markdown-cheat-sheet)__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd380092",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "We are given the following corpus, modified from the one from readings and lectures:\n",
    "```\n",
    "<s> I am Sam </s>\n",
    "<s> Sam I am </s>\n",
    "<s> I am Sam </s>\n",
    "<s> I do not like green eggs and Sam </s>\n",
    "```\n",
    "Based on this corpus, **calculate by hand** the following bigram proabilities using both (i) **maximum likelihood estimates** and (ii) **add-one smoothing**. Include ```<s>``` and ```</s>``` in your counts just like any other token.\n",
    "\n",
    "- `P(Sam | am)`\n",
    "- `P(eggs | green)`\n",
    "- `P(eggs | like)`\n",
    "\n",
    "Include your answers in the markdown cell below. You'll need to provide six estimates in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e72d75",
   "metadata": {},
   "source": [
    "**Write your answers here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30fa708",
   "metadata": {},
   "source": [
    "P(Sam | am):\n",
    "(i): 2/3\n",
    "(ii): 3/14\n",
    "\n",
    "P(eggs | green):\n",
    "(i): 1\n",
    "(ii): 1/6\n",
    "\n",
    "P(eggs | like):\n",
    "(i): 0\n",
    "(ii): 1/12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ec574",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "For this question, you will train unigram and bigram language models based by the __[Berkeley Restaurant Project](https://github.com/wooters/berp-trans/blob/master/README.md)__ (BeRP) corpus and answer questions based on those language models. Be sure to download the `transcript.txt` file from Canvas and save it in the same folder as this notebook.\n",
    "\n",
    "The code block below reads the corpus from the `transcript.txt` file and does some data cleaning.  The variable `corpus` is a list of utterances, each utterance being represented as a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b040ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "berp = open(\"oneRate.txt\", \"r\").readlines()\n",
    "berp2 = open(\"fiveRate.txt\",\"r\").readlines()\n",
    "\n",
    "corpus = []\n",
    "for line in berp:\n",
    "    line = re.sub(r'^[\\d\\w_]+\\s+', '', line).strip()  # remove audio file info\n",
    "    line = re.sub(r'<[\\w\\']+>', '', line)             # remove verbal repairs\n",
    "    line = re.sub(r'\\w+-', '', line)                  # remove fragments\n",
    "    line = re.sub(r'\\*([\\w\\']+)\\*', '\\1', line)       # remove mispronunciation markers\n",
    "    corpus.append(line.split())  \n",
    "\n",
    "corpus2 = []\n",
    "for line in berp2:\n",
    "    line = re.sub(r'^[\\d\\w_]+\\s+', '', line).strip()  # remove audio file info\n",
    "    line = re.sub(r'<[\\w\\']+>', '', line)             # remove verbal repairs\n",
    "    line = re.sub(r'\\w+-', '', line)                  # remove fragments\n",
    "    line = re.sub(r'\\*([\\w\\']+)\\*', '\\1', line)       # remove mispronunciation markers\n",
    "    corpus2.append(line.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3980a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'just', 'awful.', 'His', 'lecture', 'is', 'randomly', 'oriented,', 'he', 'is', 'alway', 'off', 'the', 'topic,', 'have', 'no', 'idea', 'what', 'he', 'was', 'talking', 'about', 'half', 'of', 'the', 'time.', 'Learn', 'nothing', 'from', 'his', 'class,', 'and', 'his', 'book', 'does', 'not', 'make', 'a', 'lot', 'of', 'sense.']\n"
     ]
    }
   ],
   "source": [
    "# View the first utterance in the corpus:\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8248cb52",
   "metadata": {},
   "source": [
    "Note that periods do not mark ends of sentences, but rather pauses longer than one second.  See the __[transcription guidelines](http://localhost:8888/edit/transcription_guidelines.txt)__ for additional details (though this is not necessary - work with corpus as given)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af3079",
   "metadata": {},
   "source": [
    "### Task 1: Unigram and bigram MLE language model\n",
    "\n",
    "Using NLTK's `FreqDist()` and `ConditionalFreqDist()` classes train unigram and bigram language models based on the BeRP corpus. Include beginning-of (`<s>`) and end-of-sentence (`</s>`) tags in your calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688ac82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1302), ('and', 976), ('to', 913), ('is', 663), ('a', 633), ('of', 587), ('are', 418), ('class', 416), ('you', 415), ('in', 400), ('not', 388), ('he', 384), ('I', 354), ('for', 338), ('on', 301), ('this', 256), ('his', 247), ('with', 247), ('was', 245), ('but', 240), ('that', 225), ('have', 204), ('He', 190), ('it', 188), ('at', 177), ('very', 174), ('lectures', 153), ('were', 153), ('if', 152), ('class.', 152), ('The', 150), ('no', 144), ('about', 142), ('be', 140), ('so', 140), ('from', 139), ('take', 136), ('professor', 133), ('your', 132), ('as', 132), ('do', 116), ('an', 114), ('she', 114), ('what', 113), ('or', 109), ('will', 107), ('just', 100), ('all', 99), ('like', 95), ('any', 95), ('students', 94), ('has', 94), (\"don't\", 90), ('even', 90), ('time', 88), ('more', 88), ('would', 88), ('her', 88), ('get', 87), ('course', 86), ('how', 86), ('homework', 83), ('because', 82), ('my', 80), ('than', 79), ('only', 79), ('had', 79), ('lecture', 78), ('ever', 76), ('His', 75), ('hard', 75), ('worst', 74), ('know', 73), (\"doesn't\", 72), ('material', 71), ('him', 70), ('make', 69), ('learn', 69), ('never', 69), ('when', 69), ('does', 66), ('one', 66), ('grade', 66), ('good', 64), ('assignments', 64), ('by', 64), ('can', 63), ('understand', 62), ('they', 62), ('teach', 61), ('really', 61), ('most', 59), ('much', 58), ('teaching', 58), ('out', 57), ('other', 57), (\"I've\", 57), ('way', 56), ('too', 56), ('extremely', 56)]\n"
     ]
    }
   ],
   "source": [
    "unigram = nltk.FreqDist()\n",
    "\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        unigram[word] += 1\n",
    "\n",
    "print(unigram.most_common(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b36c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('helpful', 3), ('well.', 3), ('helpful.', 3), ('easy', 2), ('well', 2), ('difficult', 2), ('good', 2), ('intelligent', 2), ('caring', 2), ('sharp.', 1), ('devoted', 1), ('smart', 1), ('exciting', 1), ('valuable.', 1), ('easy.', 1), ('engaging', 1), ('willing', 1), ('organized.', 1), ('organised', 1), ('approachable', 1), ('fun', 1), ('clear', 1), ('organized,', 1), ('accommodating', 1), ('straightforward,', 1), ('sassy', 1), ('professional', 1), ('\"overt\".', 1), ('personable', 1), ('lenient.', 1), ('tedious', 1), ('knowledgeable', 1), ('doable', 1), ('thorough', 1), ('clear,', 1), ('manageable', 1)]\n"
     ]
    }
   ],
   "source": [
    "bigram = nltk.ConditionalFreqDist()\n",
    "bigram2 = nltk.ConditionalFreqDist()\n",
    "\n",
    "for sentence in corpus:\n",
    "    preceding = \"<s>\"\n",
    "    for word in sentence:\n",
    "        bigram[preceding][word] += 1\n",
    "        preceding = word\n",
    "    bigram[preceding][\"</s>\"] += 1\n",
    "\n",
    "for sen in corpus2:\n",
    "    preceding = \"<s>\"\n",
    "    for word in sen:\n",
    "        bigram2[preceding][word] += 1\n",
    "        preceding = word\n",
    "    bigram2[preceding][\"</s>\"] += 1\n",
    "# bigram[\"not\"].most_common() ##################################\n",
    "most = unigram.most_common(100)\n",
    "# for i in most:\n",
    "#     print(i)\n",
    "#     print(bigram[i[0]].most_common())\n",
    "#     print(\"-------------------------------------------------\")\n",
    "\n",
    "print(bigram2['extremely'].most_common())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fbe9e",
   "metadata": {},
   "source": [
    "## Task 2: Compute the probability of a sentence\n",
    "\n",
    "Write code to compute the unigram and bigram probabilities of the sentece `i want middle eastern food` using the language models you trained in the previous question. For the bigram calculation do not forget to include the start and end of sentence symbols.  \n",
    "\n",
    "Recall that the unigram probability of a sentence $w_1...w_k$ is computed as: \n",
    "\n",
    "$$P(w_1...w_k) = \\prod_{k=1}^n P(w_k)$$\n",
    "\n",
    "and the bigram probability is computed as:\n",
    "\n",
    "$$P(w_1...w_k) = \\prod_{k=1}^n P(w_k|w_{k-1})$$\n",
    "\n",
    "Which language model assigns the sentence a higher probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8e2a3",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "When a sentence is \"i want to eat middle eastern food\", the unigram language model assigns a higher probability.\n",
    "However, when a sentence is \"i want to eat mediterranean food\", the bigram language model assigns a higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d153b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0168986865506015e-15\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "sentence = \"i want to eat middle eastern food\".split()\n",
    "\n",
    "# Write your code here.\n",
    "# Unigram probability:\n",
    "prob_uni = 1.0\n",
    "for i in sentence:\n",
    "    prob_uni *= unigram.freq(i)\n",
    "print(prob_uni)\n",
    "\n",
    "# Bigram probability:\n",
    "prob_bi = 1.0\n",
    "prec = \"<s>\"\n",
    "for i in sentence:\n",
    "    prob_bi *= bigram[prec].freq(i)\n",
    "    prec = i\n",
    "prob_bi *= bigram[prec].freq(\"</s>\")\n",
    "print(prob_bi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a4793",
   "metadata": {},
   "source": [
    "## Task 3: Corpus exploration\n",
    "\n",
    "Using your unigram and bigram language models answer the following questions.  The two code cells immediately below include calls to the `help` function on the `tabulate()` method for both frequency distributions and conditional frequency distributions. This method may be helpful in answering some of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(unigram.tabulate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(bigram.tabulate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d240c2d",
   "metadata": {},
   "source": [
    "1. The BeRP corpus includes tokens for the filled pauses `[er]`, `[mm]`, `[uh]` and `[um]`.  Which of these filled pauses occurs most frequently in the BeRP corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f76f86",
   "metadata": {},
   "source": [
    "[uh] occurs most frequently with 488 times in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd477963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[er] [mm] [uh] [um] \n",
      "   7   17  488  223 \n"
     ]
    }
   ],
   "source": [
    "filled_pauses = \"[er] [mm] [uh] [um]\".split()\n",
    "\n",
    "unigram.tabulate(samples=filled_pauses)\n",
    "# Answer: [uh] occurs most frequently with 488 times in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de861ba6",
   "metadata": {},
   "source": [
    "2. For each filled pause, tabulate the ten most frequent tokens that follow it and not any interesting differences for similarities betweens these lists.\n",
    "\n",
    "**Write your observations here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bcd3be",
   "metadata": {},
   "source": [
    "[er]: i(2 times), cost(1 time), downtown(1 time), [um](1 time), i'm(1 time), german(1 time)\n",
    "\n",
    "[mm]: <\\s>(3 times), i'd(2 times), maybe(2 times), about(2 times), relatively(2 times), i'll(1 time), medium(1 time), metropole(1 time), .(1 time), [lip_smack](1 time)\n",
    "\n",
    "[uh]: i(56 times), i'd(22 times), .(17 times), </s>(14 times), what(10 times), for(9 times), can(9 times), italian(8 times), how(7 times), a(7 times)\n",
    "\n",
    "[um]: i(29 times), .(21 times), </s>(10 times), [lip_smack](8 times), i'm(6 times), are(6 times), i'd(5 times), tell(5 imes), do(5 times), for(4 times)\n",
    "\n",
    "Similarities:\n",
    "\"I\" appears to be a common word that follows filled pauses, indicating its frequent use after a speaker pauses. Furthermore, most filled pauses seem to be used when speakers are finishing their statements.\n",
    "\n",
    "Differences:\n",
    "\"[er]\" and \"[mm]\" appear not to be used as frequently as \"[uh]\" and \"[um]\". \"[mm]\" seems to be used when a speaker is contemplating how to continue, as words like \"maybe\" and \"about\" often follow this filled pause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620104d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            i     cost downtown     [um]      i'm   german \n",
      "[er]        2        1        1        1        1        1 \n",
      "\n",
      "            </s>         i'd       maybe       about  relatively        i'll      medium   metropole           . [lip_smack] \n",
      "[mm]           3           2           2           2           2           1           1           1           1           1 \n",
      "\n",
      "           i     i'd       .    </s>    what     for     can italian     how       a \n",
      "[uh]      56      22      17      14      10       9       9       8       7       7 \n",
      "\n",
      "               i           .        </s> [lip_smack]         i'm         are         i'd        tell          do         for \n",
      "[um]          29          21          10           8           6           6           5           5           5           4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do your calculations here:\n",
    "for filled in filled_pauses:\n",
    "    most_common = bigram[filled].most_common(10)\n",
    "    li = []\n",
    "    for val in most_common:\n",
    "        li.append(val[0])\n",
    "    bigram.tabulate(conditions=[filled],samples=li)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f0060",
   "metadata": {},
   "source": [
    "3. Do the same thing as in the previous question for a silent pause, marked in the BeRP corpus with a period `.`, again nothing any interesting differences or similarities with the filled pauses.\n",
    "\n",
    "**Write your observations here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e5b95",
   "metadata": {},
   "source": [
    "\".\": i(57 times), and(39 times), for(28 times), [uh](24 times), a(22 times), to(19 times), on(16 times), the(14 times), over(14 times), [um](13 times) \n",
    "\n",
    "Observation:\n",
    "Similar to filled pauses, \"I\" frequently follows the silent pause. Moreover, observing that some filled pauses follow the silent pause reveals that it is sometimes succeeded by hesitation. Another important observation is that the silent pause is frequently used when a speaker attempts to make a new statement. This is because \"i\" was the word that most frequently followed the silent pause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e207622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     i  and  for [uh]    a   to   on  the over [um] \n",
      ".   57   39   28   24   22   19   16   14   14   13 \n"
     ]
    }
   ],
   "source": [
    "# Do your calculations here\n",
    "most_common = bigram[\".\"].most_common(10)\n",
    "li = []\n",
    "for val in most_common:\n",
    "    li.append(val[0])\n",
    "bigram.tabulate(conditions=[\".\"],samples=li)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
